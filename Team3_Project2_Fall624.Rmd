---
title: "DATA 624 Fall 2017: Project 2"
author: "Team 3: Michael Lennon, Ken Markus, Albania Nicasio, Dan Smilowitz, Logan Thomson"
date: "December 12, 2017"
output: 
  word_document: 
    fig_height: 5
    fig_width: 7
    highlight: pygments
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE,
                      comment = NA, fig.align = "center")
```  

# Data Exploration  

```{r load-data, eval=FALSE, echo=FALSE}
# read in data
library(readxl)

ph_data <- read_excel("StudentData.xlsx")
```  

```{r load-data-github, eval=FALSE}
# use if we want to load data from GitHub

# open file
path <- ("https://raw.githubusercontent.com/kennygfm/fall624-group3/master/StudentData.csv")
con <- file(path, open="r")

# "Student" soft drink data
ph_data <- read.csv(con, header=T, sep=",", stringsAsFactors = F)

# close file
close(con)

ph_data[ , c(16,18,21,28)] <- sapply(ph_data[, c(16,18,21,28)], as.numeric)  # get rid of pesky integer values
```  

The names in the source file contain spaces; these are replaced with periods to make data manipulation simpler.  The structure of the data is presented below:  

```{r rename-str}
library(stringr)

names(ph_data) <- str_replace_all(names(ph_data), " ", ".")
str(ph_data)
```  

The data contains 2571 observations across 33 variables.  The first variable, `Brand.Code`, is a character; the remaining 32 variables are numeric.  Summary statistics of the variables are presented below:  

```{r summary} 
# summary(ph_data)  # use this or the summary table below?
```  

```{r summary_table}
library(e1071)

means <- sapply(ph_data[-1], function(y) mean(y, na.rm = TRUE))
medians <- sapply(ph_data[-1], function(y) median(y, na.rm = TRUE))
IQRs <- sapply(ph_data[-1], function(y) IQR(y, na.rm = TRUE))
vars <- sapply(ph_data[-1], function(y) var(y, na.rm = TRUE))
skews <- sapply(ph_data[-1], function(y) skewness(as.numeric(y), na.rm = TRUE))
cors <- as.vector(cor(ph_data$PH, ph_data[,2:ncol(ph_data)], use = "complete.obs"))
NAs <- sapply(ph_data[-1], function(y) sum(length(which(is.na(y)))))

soda_summary <- data.frame(means, medians, IQRs, vars, skews, cors, NAs)
colnames(soda_summary) <- c("MEAN", "MEDIAN", "IQR", "Var", "SKEW", "$r_{PH}$", "NAs")
soda_summary <- round(soda_summary, 2)

pander(soda_summary)
```  

## Missing Values  

As shown in the above summary, there are missing values across the variables -- the frequency and pattern of these missing values are presented below:  

```{r missing-pattern}
library(VIM)

aggr(ph_data, sortVars = TRUE, bar = FALSE, prop = FALSE, gap = 1, cex.axis = 0.7,
     col = c("navyblue", "yellow"), ylab = c("Number Missing", "Pattern"))
```  

The variable `MFR` has over 200 missing values, and the variable `Brand.Code` is missing 120 values.  Due to these high proportions of missingness, observations missing these variables are dropped:  

```{r drop-na}
library(plyr) # loaded for later dependecies to avoid conflicts with dplyr
library(tidyverse)

ph_data <- drop_na(ph_data, MFR, Brand.Code)
```  

Numeric variables are plotted below to determine the best method of imputation:  

```{r var-hist}
theme_set(theme_light())

# histograms

ph_data %>% 
  select(-Brand.Code) %>% 
  gather(Variable, Values) %>% 
  ggplot(aes(x = Values)) +
  geom_histogram(alpha = 0.25, col = "black", bins = 20) +
  facet_wrap(~ Variable, scales = "free", nrow = 4)
```  

Many of the variables have normal or somewhat-normal distributions, and appear to be continuous variables. Many of the variables show varying levels of skewness, and a few of the skewed distributions follow an almost chi-squared or log-normal distribution. These are predictors (`PSC.Distribution` and `PSC.C02.Distribution`) where transformations should be considered. 

Another insteresting pattern are the number of 0 values in the `Hyd.Pressure`(1-3) variables.  The $4^{th}$ `Hyd.Pressure` does not follow this same pattern, so depending on the relationship between these variables and the target and/or other variables, they may be removed outright.  

Some of the predictors appear to be discrete, rather than continuous distributions (`Pressure.Setpoint`, `Alch.Rel`), however, many of these variables are just constrained to a few values, but are still continuous.  Out of all the variables, `Bowl.Setpoint` does seem to be a discrete distribution. A table of all values in the variable is below:    

```{r bwl_stpt_vals}
table(ph_data$Bowl.Setpoint)
```  

```{r bowl_setpt_dist}
# histogram may be redundant
ph_data %>% 
  select(Bowl.Setpoint) %>% 
  gather(Variable, Values) %>% 
  ggplot(aes(x = Values)) +
  geom_histogram(alpha = 0.25, col = "black", bins = 20) +
  facet_wrap(~ Variable, scales = "free", nrow = 4)
```  

Boxplots provide an alternate view that allows the spread of the data to be viewed a little more clearly, as well as the presence of outliers and their quantities:  

```{r}
# boxplots

ph_data %>% 
  select(-Brand.Code) %>% 
  gather(Variable, Values) %>% 
  ggplot(aes(x = 1, y = Values)) +
  geom_boxplot() +
  facet_wrap(~ Variable, scales = "free", nrow = 4)
```  

A number of these variables are so highly skewed, such as `Filler.Speed`, `Oxygen.Filler`, `MFR`, and `Air.Pressurer`, that many of the observations for that predictor are recognized as outliers. This suggests that imputing with the mean will not be accurate; an alternate method should be investigated.  

Before moving on to correlations between predictors, the relationship between the target variable `PH` and the other predictors is visualized using scatterplots:  

```{r scat_plts}
# boxplots

ph_data %>% 
  gather(-PH, -Brand.Code, key="Var", value="Value") %>% 
  ggplot(aes(x=Value, y=PH, color=Brand.Code)) + geom_point(alpha=0.6) +
  facet_wrap(~ Var, scales = "free", nrow=4)
```  

Here we see there are no strong patterns (i.e. no defined linear pattern), but there are some relationships between `PH` and the other variables that may be more helpful in predicting the `PH` level of the soft drink.  Predictors like `Alch.Rel`, `Density`, and `Temperature` all appear to have a stronger relationshp with `PH` - this makes sense, as these variables have more to do with the make-up of the beverage itself, rather than the filling/bottling process.  

Also interesting is the clustering of the `Brand.Code` within the plots. As correlations are investigated, this predictor does not have a strong relationship with the target, but combined with other predictors, may be more telling.  

## Correlation  

The correlation between the variables is investigated:  

```{r cor}
ph_cors <- cor(ph_data %>% select(-Brand.Code), use="complete.obs")
library(corrplot)
corrplot(as.matrix(ph_cors), method = "color", tl.cex = 0.5, tl.col = "black")
```  

As the columns are organized in the data, some interesting patterns are present in the correlogram. Two areas show distinct positive correlations -- these are the predictors that have something to do with carbonation, and another area where different pressure levels correlate with each other. Another set of variables are negatively correlated with these pressure predictors, these have to do with the filling of the bottles, so this makes sense (`Oxygen.Filler`, `Bowl.Setpoint`, `Pressure.Setpoint`).

Some of these same predictors are also correlated well with the target PH variable. The top positive correlations are below:  

```{r top-ph-cors}
library(pander)
top_ph_cors <- ph_cors %>% 
  as.data.frame() %>% 
  select(Correlation = PH) %>% 
  rownames_to_column("Variable") %>% 
  arrange(desc(Correlation))

top_ph_cors %>%
  top_n(11, Correlation) %>% 
  pander()
```  

The predictors with the highest negative correlation to `PH` are as follows:  

```{r}
top_ph_cors %>%
  top_n(-11, Correlation) %>%
  arrange(Correlation) %>%
  pander()
```  

###Intercorrelated Predictors  

In addition, some of the variables are highly correlated with each other. The following pairings seem to have something to do with each other in the bottle filling process:  

```{r inter_corr}
library(reshape2)
ph_cors_tri <- ph_cors

ph_cors_tri[lower.tri(ph_cors_tri, diag = TRUE)] <- NA  # prevent duplicates by taking upper triangle of matrix

ph_cors_tri %>% 
  melt() %>%
  as.data.frame() %>%
  filter(!is.na(value)) %>%
  arrange(desc(value)) %>%
  top_n(10) %>%
  pander()
```  

Among the variables with postive correlations with each other, many of them are almost perfectly correlated with one another. We may want to consider removing some of these redundant variables, perhaps the ones that are less correlated to the target variable.  In addition predictors with high negative correlations with each other are below:  

```{r int_cor_neg}
ph_cors_tri %>% 
  melt() %>%
  as.data.frame() %>%
  filter(!is.na(value)) %>%
  arrange(value) %>%
  top_n(-10) %>%
  pander()
```  

Three similarly-names variables, `Hyd.Pressure1`, `Hyd.Pressure2`, and `Hyd.Pressure3`show large spikes at values of 0 in the histograms above, and show high correlations.  This suggests that these variables are candidates for removal.  These variables, along with a fourth related variable `Hyd.Pressure4`, are first investigated across the four `Brand.Code` values:  

```{r pressure-brand}

ph_data %>% 
  select(Brand.Code, Hyd.Pressure1:Hyd.Pressure4) %>% 
  gather(HydNum, Value, -Brand.Code) %>% 
  ggplot(aes(x = Value)) +
  geom_histogram(bins = 25) +
  facet_grid(Brand.Code ~ HydNum, scales = "free") +
  theme(panel.grid = element_blank()) +
  scale_x_continuous(NULL, NULL, NULL) +
  scale_y_continuous(NULL, NULL, NULL) +
  ggtitle("Distribution of Hyd.Pressure variables across Brand.Codes")
```  

These distributions suggest that there is no behavior in `Hyd.Pressure1`, `Hyd.Pressure2`, or `Hyd.Pressure3` indicated by `Brand.Code`.  For this reason, these three variables are dropped.  `Hyd.Pressure4` is retained, as there appear to be differences in its behavior across `Brand.Code`:  

```{r drop-pressure}
ph_data <- ph_data %>% select(-(Hyd.Pressure1:Hyd.Pressure3))
```  


# Data Preprocessing
Before imputation, the data is split into predictors and a response:
```{r train-test}
ph_pred <- ph_data %>% select(-PH)
ph_resp <- ph_data %>% select(PH)
```


## Imputation
Due to the skewness of and relationship between predictors, they are imputed using k-nearest neighbors.  Prior to this imputation, predictors are centered and scaled to avoid bias in predictive models, and highly-correlated predictors are removed.  The `preProcess` function from the `caret` package is capable of performing all of these operations -- per the documentation for this function:

  > The operations are applied in this order: zero-variance filter, near-zero variance filter, correlation filter, Box-Cox/Yeo-Johnson/exponential transformation, centering, scaling, range, imputation, PCA, ICA then spatial sign.

```{r preprocess}
library(caret)
# set up pre-processing transformation
ph_preproc <- preProcess(ph_pred, method = c("knnImpute", "center", "scale", "corr"))
# apply pre-processing to data
ph_pred <- predict(ph_preproc, ph_pred)
```


## Partitioning
With pre-precessing complete, both predictor and response data are partitioned into a training and testing set:
```{r train-test}
# get rows for training subsets
set.seed(100)  # for replicability
train_rows <- createDataPartition(ph_resp$PH, p = 0.75, list = FALSE)
# create training sets
ph_pred_train <- ph_pred[train_rows, ]
ph_resp_train <- ph_resp[train_rows, ]
# creae test sets
ph_pred_test <- ph_pred[-train_rows, ]
ph_resp_test <- ph_resp[-train_rows, ]
```



## Model Creation
The pre-processed data is used to fit an array of models: linear models; non-linear models; and tree based models.  For consistency, the same training controls are used for all models:
```{r train-control}
# use 15-fold cross-validation for training
set.seed(100)
mdl_ctrl <- trainControl(method = "cv", number = 15)
```

## Linear Models
```{r eval=FALSE}
#Libraries used:
library(pls)
library(leaps)
```

```{r eval=FALSE}
##Numeric Brand.Code for training
bc <- ph_pred_train$Brand.Code
bc <- as.character(bc)
bc[bc=='A'] <-1
bc[bc=='B'] <-2
bc[bc=='C'] <-3
bc[bc=='D'] <-4
bc <- as.numeric(bc)
ph_pred_train_lm <- ph_pred_train
ph_pred_train_lm$Brand.Code <- bc


## Change Brand.Code for test set

ph_pred_test_lm <- ph_pred_test
ph_pred_test_lm$Brand.Code <- as.character(ph_pred_test_lm$Brand.Code)
ph_pred_test_lm$Brand.Code[ph_pred_test_lm$Brand.Code=='A'] <-1
ph_pred_test_lm$Brand.Code[ph_pred_test_lm$Brand.Code=='B'] <-2
ph_pred_test_lm$Brand.Code[ph_pred_test_lm$Brand.Code=='C'] <-3
ph_pred_test_lm$Brand.Code[ph_pred_test_lm$Brand.Code=='D'] <-4
ph_pred_test_lm$Brand.Code <- as.numeric(ph_pred_test_lm$Brand.Code)



regressData <- data.frame(ph_pred_train_lm,ph_resp_train, stringsAsFactors = FALSE)
lmTune<-train(PH ~., data = regressData, method = "lm")
summary(lmTune) # The R Squared was only .37 
lmPred <- predict(lmTune, newdata = ph_pred_test_lm)
lmPerf <- postResample(pred = lmPred, obs = unlist(ph_resp_test) )


################# GLM Regression ######
glmTune<-glm(PH ~., data=regressData, family="quasipoisson")  
summary(glmTune) # 
glmPred <- predict(glmTune, newdata = ph_pred_test_lm)
glmPerf <- postResample(pred = glmPred, obs = unlist(ph_resp_test) )
# Then best glm family tested was gaussian 


################# Principal Components Regression ######
pcrTune <- pcr(PH~., data = regressData, scale = TRUE, validation = "CV")
summary(pcrTune)
validationplot(pcrTune, val.type="RMSE")
validationplot(pcrTune, val.type = "R2")

# Then we choose 18 Principal Components
pcrPred <- predict(pcrTune, newdata = ph_pred_test_lm, ncomp=18)
pcrPerf <- postResample(pred = pcrPred, obs = unlist(ph_resp_test) )
# There are too many principal components in this model - making it difficult to interpret which each component represents.

################# Stepwise Backwards Linear Regression ######
# The full linear regression had some variables with high p values
# No variables
null <-  lm(PH ~1, data = regressData)

# All variables
full <- lm(PH ~., data = regressData)

stepTune <- step(full, data=regressData, direction="backward")
summary(stepTune)
# The variables chosen for inclusion were 
# Brand.Code , Fill.Ounces, PSC.Fill, Mnf.Flow, Carb.Pressure1, Hyd.Pressure4, Filler.Level, Temperature, Usage.cont, Carb.Flow          
# Density, MFR, Pressure.Vacuum, Oxygen.Filler, Pressure.Setpoint, Carb.Rel 
stepPred <- predict(stepTune, newdata = ph_pred_test_lm)
stepPerf <- postResample(pred = stepPred, obs = unlist(ph_resp_test) )
########################

```

## Non-Linear Models
VARIABLE NAMES UPDATED PER KEN
```{r eval=FALSE}
#MARS
library(earth)
marsFit <- earth(x = ph_pred_train, y = ph_resp_train)

marsPred <- predict(marsFit, newdata = ph_pred_test)
marsPerf <- postResample(pred = marsPred, obs = ph_resp_test)

#Now let's do NN
library(nnet)
nnetGrid <- expand.grid(.decay = c(0, 0.01, .1),
                        .size = c(1:10),
                        .bag = FALSE)
set.seed(100)
nnetTune <- train(x = ph_pred_train, y = ph_resp_train,
                  method = "avNNet",
                  tuneGrid = nnetGrid,
                  preProc = c("center", "scale"),
                  linout = TRUE,
                  trace = FALSE,
                  maxit = 50)
nnetPred <- predict(nnetTune, newdata = ph_pred_test)
nnetPerf <- postResample(pred = nnetPred, obs = ph_resp_test) 


#Now let's do SVM
library(kernlab)

#Note that we have to convert Brand.Code to numeric or remove...
bc <- ph_pred_train$Brand.Code
bc <- as.character(bc)
bc[bc=='A'] <-1
bc[bc=='B'] <-2
bc[bc=='C'] <-3
bc[bc=='D'] <-4
bc <- as.numeric(bc)
ph_pred_train_svm <- ph_pred_train
ph_pred_train_svm$Brand.Code <- bc

#test data BrandCode needs to be updated too
ph_pred_test_svm <- ph_pred_test
ph_pred_test_svm$Brand.Code <- as.character(ph_pred_test_svm$Brand.Code)
ph_pred_test_svm$Brand.Code[ph_pred_test_svm$Brand.Code=='A'] <-1
ph_pred_test_svm$Brand.Code[ph_pred_test_svm$Brand.Code=='B'] <-2
ph_pred_test_svm$Brand.Code[ph_pred_test_svm$Brand.Code=='C'] <-3
ph_pred_test_svm$Brand.Code[ph_pred_test_svm$Brand.Code=='D'] <-4
ph_pred_test_svm$Brand.Code <- as.numeric(ph_pred_test_svm$Brand.Code)

#Radial first
set.seed(100)
svmTune <- train(x = ph_pred_train_svm, y = ph_resp_train,
                 method = "svmRadial", 
                 tuneLength = 14,
                 preProc = c("center", "scale"),
                 trControl = trainControl(method = "cv"))

svmRadialPred <- predict(svmTune, newdata = ph_pred_test_svm)
svmRadialPerf <- postResample(pred = svmRadialPred, obs = ph_resp_test) 

#Linear first
set.seed(100)
svmTune <- train(x = ph_pred_train_svm, y = ph_resp_train,
                 method = "svmLinear", 
                 tuneLength = 14,
                 preProc = c("center", "scale"),
                 trControl = trainControl(method = "cv"))

svLinearPred <- predict(svmTune, newdata = ph_pred_test_svm)
svmLinearPerf <- postResample(pred = svLinearPred, obs = ph_resp_test) 

#KNN 
library(caret)
set.seed(100)
#As with SVM we must use training data that converts Brand.Code to numeric
knnModel <- train(x = ph_pred_train_svm, y = ph_resp_train, method = "knn", 
                  tuneLength = 10)
knnModel #max(knnModel$results$Rsquared) 0.2349691
knnPred <- predict(knnModel, newdata = ph_pred_test_svm) 
knnPerf <- postResample(pred = knnPred, obs = ph_resp_test)
```


## Tree-Based Models
##For this project we will see the constraints we encountered with our missing variables and the multiple regression. In the real world, the best model can be only as good as the variables measured by the study. We should also keep in mind that P-values can change based on the specific terms in the model. We can also appreciate that Stepwise regression and best subsets regression are great tools and can get you close to the correct model. However, studies have found that they generally don’t pick the correct model. With all these complications we agreed that Tree-based methods are simple and useful for interpretation, but also Combining many trees can often result in dramatic improvements in prediction accuracy, at the expense of some loss interpretation.
For this project we use 6 models below.


## Conventional Tree Model
```{r tree}
library(rpart)
# conventional tree of max depth
set.seed(100)
rpartTune <- train(x = ph_pred_train, y = ph_resp_train$PH,
                   method = "rpart2", trControl = mdl_ctrl)
rpartPred <- predict(rpartTune, newdata = ph_pred_test)
rpartPerf <- postResample(pred = rpartPred, obs = ph_resp_test$PH)
```


## Rule-Based Model
```{r rule-tree}
library(RWeka)
set.seed(100)
ruleTune <- train(x = ph_pred_train, y = ph_resp_train$PH,
                  method = "M5Rules", trControl = mdl_ctrl)
rulePred <- predict(ruleTune, newdata = ph_pred_test)
rulePerf <- postResample(pred = rulePred, obs = ph_resp_test)
```


## Bagged Tree Model
```{r bagged-tree}
library(ipred)
set.seed(100)
bagTune <- train(x = ph_pred_train, y = ph_resp_train$PH,
                 method = "treebag", trControl = mdl_ctrl)
bagPred <- predict(bagTune, newdata = ph_pred_test)
bagPerf <- postResample(pred = bagPred, obs = ph_resp_test$PH)
```


## Random Forest Model
```{r rf}
library(randomForest)
set.seed(100)
rfTune <- train(x = ph_pred_train, y = ph_resp_train$PH,
                method = "rf", trControl = mdl_ctrl,
                ntrees = 1000, importance = TRUE)
rfPred <- predict(rfTune, newdata = ph_pred_test)
rfPerf <- postResample(pred = rfPred, obs = ph_resp_test$PH)
```


## Boosted Tree Model
```{r boosted-tree}
library(gbm)
set.seed(100)
boostTune <- train(x = ph_pred_train, y = ph_resp_train$PH,
                   method = "gbm", trControl = mdl_ctrl,
                   tuneGrid = expand.grid(shrinkage = c(0.01, 0.05, 0.1),
                                          interaction.depth = seq(1, 9, 2),
                                          n.trees = seq(100, 1000, 100),
                                          n.minobsinnode = 10),
                   verbose = FALSE)
boostPred <- predict(boostTune, newdata = ph_pred_test)
boostPerf <- postResample(pred = boostPred, obs = ph_resp_test$PH)
```


## Cubist Model
```{r cubist}
library(Cubist)
set.seed(100)
cubistTune <- train(x = ph_pred_train, y = ph_resp_train,
                    method = "cubist", trControl = mdl_ctrl,
                    tuneGrid = expand.grid(neighbors = c(0, 1, 5, 9),
                                           committees = c(1, 25, 50, 75, 100)))
cubistPred <- predict(cubistTune, newdata = ph_pred_test)
cubistPerf <- postResample(pred = cubistPred, obs = ph_resp_test)

```


# Model Selection & Prediction
## Model Performance Comparison
The resampled RMSE & RMSE performance against the test set is shown below for each of the models created:
```{r compare}
model_perf <- function (model_set, metric = "RMSE") {
  mdl_names <- character()
  resampled <- numeric()
  test <- numeric()
  for (mdl in model_set) {
    mdl_names <- c(mdl_names, mdl)
    resampled <- c(resampled, min(get(paste0(mdl, "Tune"))$results[[metric]]))
    test <- c(test, get(paste0(mdl, "perf"))[metric])
  }
  pander(data.frame(`Resampled RMSE` = resampled, `Test RMSE` = test,
                    row.names = mdl_names, check.names = FALSE), digits = 4)
}

model_perf(c("rpart", "rule", "bag", "rf", "boost", "cubist"))
```


## Predictor Importance
ONCE WE PICK A MODEL


## Prediction of Future Data
ONCE WE PICK A MODEL
